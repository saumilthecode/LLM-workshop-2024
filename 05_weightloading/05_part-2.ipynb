{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0599d57c-16e4-478c-954d-89dbbd193ced",
   "metadata": {},
   "source": [
    "**LLM Workshop 2024 by Sebastian Raschka**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14b82151-07c6-4867-b374-741258033b52",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "# 5) Loading pretrained weights (part 2; using LitGPT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d617b8f-8493-4afa-8c91-f3d1ab79795b",
   "metadata": {},
   "source": [
    "- Now, we are loading the weights using an open-source library called LitGPT\n",
    "- LitGPT is fundamentally similar to the LLM code we implemented previously, but it is much more sophisticated and supports more than 20 different LLMs (Mistral, Gemma, Llama, Phi, and more)\n",
    "\n",
    "# ⚡ LitGPT\n",
    "\n",
    "**20+ high-performance LLMs with recipes to pretrain, finetune, deploy at scale.**\n",
    "\n",
    "<pre>\n",
    "✅ From scratch implementations     ✅ No abstractions    ✅ Beginner friendly   \n",
    "✅ Flash attention                  ✅ FSDP               ✅ LoRA, QLoRA, Adapter\n",
    "✅ Reduce GPU memory (fp4/8/16/32)  ✅ 1-1000+ GPUs/TPUs  ✅ 20+ LLMs            \n",
    "</pre>\n",
    "\n",
    "## Basic usage:\n",
    "\n",
    "```\n",
    "# ligpt [action] [model]\n",
    "litgpt  download  meta-llama/Meta-Llama-3-8B-Instruct\n",
    "litgpt  chat      meta-llama/Meta-Llama-3-8B-Instruct\n",
    "litgpt  evaluate  meta-llama/Meta-Llama-3-8B-Instruct\n",
    "litgpt  finetune  meta-llama/Meta-Llama-3-8B-Instruct\n",
    "litgpt  pretrain  meta-llama/Meta-Llama-3-8B-Instruct\n",
    "litgpt  serve     meta-llama/Meta-Llama-3-8B-Instruct\n",
    "```\n",
    "\n",
    "\n",
    "- You can learn more about LitGPT in the [corresponding GitHub repository](https://github.com/Lightning-AI/litgpt), that contains many tutorials, use cases, and examples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b1f9508e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: litgpt in /opt/conda/envs/LLMs/lib/python3.10/site-packages (0.4.13)\n",
      "Requirement already satisfied: torch>=2.2.0 in /opt/conda/envs/LLMs/lib/python3.10/site-packages (from litgpt) (2.4.1)\n",
      "Requirement already satisfied: numpy<2.0 in /opt/conda/envs/LLMs/lib/python3.10/site-packages (from litgpt) (1.26.4)\n",
      "Requirement already satisfied: lightning==2.4.0.dev20240728 in /opt/conda/envs/LLMs/lib/python3.10/site-packages (from litgpt) (2.4.0.dev20240728)\n",
      "Requirement already satisfied: jsonargparse<=4.32.1,>=4.30.1 in /opt/conda/envs/LLMs/lib/python3.10/site-packages (from jsonargparse[signatures]<=4.32.1,>=4.30.1->litgpt) (4.32.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.23.5 in /opt/conda/envs/LLMs/lib/python3.10/site-packages (from litgpt) (0.25.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /opt/conda/envs/LLMs/lib/python3.10/site-packages (from litgpt) (0.4.5)\n",
      "Requirement already satisfied: tokenizers>=0.15.2 in /opt/conda/envs/LLMs/lib/python3.10/site-packages (from litgpt) (0.20.0)\n",
      "Requirement already satisfied: tqdm>=4.66.0 in /opt/conda/envs/LLMs/lib/python3.10/site-packages (from litgpt) (4.66.5)\n",
      "Requirement already satisfied: PyYAML<8.0,>=5.4 in /opt/conda/envs/LLMs/lib/python3.10/site-packages (from lightning==2.4.0.dev20240728->litgpt) (6.0.2)\n",
      "Requirement already satisfied: fsspec<2026.0,>=2022.5.0 in /opt/conda/envs/LLMs/lib/python3.10/site-packages (from fsspec[http]<2026.0,>=2022.5.0->lightning==2.4.0.dev20240728->litgpt) (2024.6.1)\n",
      "Requirement already satisfied: lightning-utilities<2.0,>=0.10.0 in /opt/conda/envs/LLMs/lib/python3.10/site-packages (from lightning==2.4.0.dev20240728->litgpt) (0.11.7)\n",
      "Requirement already satisfied: packaging<25.0,>=20.0 in /opt/conda/envs/LLMs/lib/python3.10/site-packages (from lightning==2.4.0.dev20240728->litgpt) (24.1)\n",
      "Requirement already satisfied: torchmetrics<3.0,>=0.7.0 in /opt/conda/envs/LLMs/lib/python3.10/site-packages (from lightning==2.4.0.dev20240728->litgpt) (1.4.2)\n",
      "Requirement already satisfied: typing-extensions<6.0,>=4.4.0 in /opt/conda/envs/LLMs/lib/python3.10/site-packages (from lightning==2.4.0.dev20240728->litgpt) (4.12.2)\n",
      "Requirement already satisfied: pytorch-lightning in /opt/conda/envs/LLMs/lib/python3.10/site-packages (from lightning==2.4.0.dev20240728->litgpt) (2.4.0)\n",
      "Requirement already satisfied: filelock in /opt/conda/envs/LLMs/lib/python3.10/site-packages (from huggingface-hub>=0.23.5->litgpt) (3.16.1)\n",
      "Requirement already satisfied: requests in /opt/conda/envs/LLMs/lib/python3.10/site-packages (from huggingface-hub>=0.23.5->litgpt) (2.32.3)\n",
      "Requirement already satisfied: docstring-parser>=0.15 in /opt/conda/envs/LLMs/lib/python3.10/site-packages (from jsonargparse[signatures]<=4.32.1,>=4.30.1->litgpt) (0.16)\n",
      "Requirement already satisfied: typeshed-client>=2.1.0 in /opt/conda/envs/LLMs/lib/python3.10/site-packages (from jsonargparse[signatures]<=4.32.1,>=4.30.1->litgpt) (2.7.0)\n",
      "Requirement already satisfied: sympy in /opt/conda/envs/LLMs/lib/python3.10/site-packages (from torch>=2.2.0->litgpt) (1.13.3)\n",
      "Requirement already satisfied: networkx in /opt/conda/envs/LLMs/lib/python3.10/site-packages (from torch>=2.2.0->litgpt) (3.3)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/envs/LLMs/lib/python3.10/site-packages (from torch>=2.2.0->litgpt) (3.1.4)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /opt/conda/envs/LLMs/lib/python3.10/site-packages (from torch>=2.2.0->litgpt) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /opt/conda/envs/LLMs/lib/python3.10/site-packages (from torch>=2.2.0->litgpt) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /opt/conda/envs/LLMs/lib/python3.10/site-packages (from torch>=2.2.0->litgpt) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /opt/conda/envs/LLMs/lib/python3.10/site-packages (from torch>=2.2.0->litgpt) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /opt/conda/envs/LLMs/lib/python3.10/site-packages (from torch>=2.2.0->litgpt) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /opt/conda/envs/LLMs/lib/python3.10/site-packages (from torch>=2.2.0->litgpt) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /opt/conda/envs/LLMs/lib/python3.10/site-packages (from torch>=2.2.0->litgpt) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /opt/conda/envs/LLMs/lib/python3.10/site-packages (from torch>=2.2.0->litgpt) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /opt/conda/envs/LLMs/lib/python3.10/site-packages (from torch>=2.2.0->litgpt) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /opt/conda/envs/LLMs/lib/python3.10/site-packages (from torch>=2.2.0->litgpt) (2.20.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /opt/conda/envs/LLMs/lib/python3.10/site-packages (from torch>=2.2.0->litgpt) (12.1.105)\n",
      "Requirement already satisfied: triton==3.0.0 in /opt/conda/envs/LLMs/lib/python3.10/site-packages (from torch>=2.2.0->litgpt) (3.0.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /opt/conda/envs/LLMs/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=2.2.0->litgpt) (12.6.68)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /opt/conda/envs/LLMs/lib/python3.10/site-packages (from fsspec[http]<2026.0,>=2022.5.0->lightning==2.4.0.dev20240728->litgpt) (3.10.8)\n",
      "Requirement already satisfied: setuptools in /opt/conda/envs/LLMs/lib/python3.10/site-packages (from lightning-utilities<2.0,>=0.10.0->lightning==2.4.0.dev20240728->litgpt) (75.1.0)\n",
      "Requirement already satisfied: importlib-resources>=1.4.0 in /opt/conda/envs/LLMs/lib/python3.10/site-packages (from typeshed-client>=2.1.0->jsonargparse[signatures]<=4.32.1,>=4.30.1->litgpt) (6.4.5)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/envs/LLMs/lib/python3.10/site-packages (from jinja2->torch>=2.2.0->litgpt) (2.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/envs/LLMs/lib/python3.10/site-packages (from requests->huggingface-hub>=0.23.5->litgpt) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/envs/LLMs/lib/python3.10/site-packages (from requests->huggingface-hub>=0.23.5->litgpt) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/envs/LLMs/lib/python3.10/site-packages (from requests->huggingface-hub>=0.23.5->litgpt) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/envs/LLMs/lib/python3.10/site-packages (from requests->huggingface-hub>=0.23.5->litgpt) (2024.8.30)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/envs/LLMs/lib/python3.10/site-packages (from sympy->torch>=2.2.0->litgpt) (1.3.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /opt/conda/envs/LLMs/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning==2.4.0.dev20240728->litgpt) (2.4.2)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/envs/LLMs/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning==2.4.0.dev20240728->litgpt) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/envs/LLMs/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning==2.4.0.dev20240728->litgpt) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/envs/LLMs/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning==2.4.0.dev20240728->litgpt) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/envs/LLMs/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning==2.4.0.dev20240728->litgpt) (6.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.12.0 in /opt/conda/envs/LLMs/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning==2.4.0.dev20240728->litgpt) (1.13.1)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/envs/LLMs/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning==2.4.0.dev20240728->litgpt) (4.0.3)\n"
     ]
    }
   ],
   "source": [
    "%pip install litgpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "48cf71fa-af17-4c72-a6ab-f258a2b5a8ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "litgpt version: 0.4.13\n",
      "torch version: 2.4.1\n"
     ]
    }
   ],
   "source": [
    "from importlib.metadata import version\n",
    "\n",
    "pkgs = [\"litgpt\", \n",
    "        \"torch\",\n",
    "       ]\n",
    "for p in pkgs:\n",
    "    print(f\"{p} version: {version(p)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe29baa9-c3b0-493d-94b4-eaa8146d6b3c",
   "metadata": {},
   "source": [
    "- First, let's see what LLMs are supported"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0ae8df66-f391-4266-b437-a1f601a6ac40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please specify --repo_id <repo_id>. Available values:\n",
      "codellama/CodeLlama-13b-hf\n",
      "codellama/CodeLlama-13b-Instruct-hf\n",
      "codellama/CodeLlama-13b-Python-hf\n",
      "codellama/CodeLlama-34b-hf\n",
      "codellama/CodeLlama-34b-Instruct-hf\n",
      "codellama/CodeLlama-34b-Python-hf\n",
      "codellama/CodeLlama-70b-hf\n",
      "codellama/CodeLlama-70b-Instruct-hf\n",
      "codellama/CodeLlama-70b-Python-hf\n",
      "codellama/CodeLlama-7b-hf\n",
      "codellama/CodeLlama-7b-Instruct-hf\n",
      "codellama/CodeLlama-7b-Python-hf\n",
      "databricks/dolly-v2-12b\n",
      "databricks/dolly-v2-3b\n",
      "databricks/dolly-v2-7b\n",
      "EleutherAI/pythia-1.4b\n",
      "EleutherAI/pythia-1.4b-deduped\n",
      "EleutherAI/pythia-12b\n",
      "EleutherAI/pythia-12b-deduped\n",
      "EleutherAI/pythia-14m\n",
      "EleutherAI/pythia-160m\n",
      "EleutherAI/pythia-160m-deduped\n",
      "EleutherAI/pythia-1b\n",
      "EleutherAI/pythia-1b-deduped\n",
      "EleutherAI/pythia-2.8b\n",
      "EleutherAI/pythia-2.8b-deduped\n",
      "EleutherAI/pythia-31m\n",
      "EleutherAI/pythia-410m\n",
      "EleutherAI/pythia-410m-deduped\n",
      "EleutherAI/pythia-6.9b\n",
      "EleutherAI/pythia-6.9b-deduped\n",
      "EleutherAI/pythia-70m\n",
      "EleutherAI/pythia-70m-deduped\n",
      "garage-bAInd/Camel-Platypus2-13B\n",
      "garage-bAInd/Camel-Platypus2-70B\n",
      "garage-bAInd/Platypus-30B\n",
      "garage-bAInd/Platypus2-13B\n",
      "garage-bAInd/Platypus2-70B\n",
      "garage-bAInd/Platypus2-70B-instruct\n",
      "garage-bAInd/Platypus2-7B\n",
      "garage-bAInd/Stable-Platypus2-13B\n",
      "google/codegemma-7b-it\n",
      "google/gemma-2-27b\n",
      "google/gemma-2-27b-it\n",
      "google/gemma-2-2b\n",
      "google/gemma-2-2b-it\n",
      "google/gemma-2-9b\n",
      "google/gemma-2-9b-it\n",
      "google/gemma-2b\n",
      "google/gemma-2b-it\n",
      "google/gemma-7b\n",
      "google/gemma-7b-it\n",
      "h2oai/h2o-danube2-1.8b-chat\n",
      "keeeeenw/MicroLlama\n",
      "lmsys/longchat-13b-16k\n",
      "lmsys/longchat-7b-16k\n",
      "lmsys/vicuna-13b-v1.3\n",
      "lmsys/vicuna-13b-v1.5\n",
      "lmsys/vicuna-13b-v1.5-16k\n",
      "lmsys/vicuna-33b-v1.3\n",
      "lmsys/vicuna-7b-v1.3\n",
      "lmsys/vicuna-7b-v1.5\n",
      "lmsys/vicuna-7b-v1.5-16k\n",
      "meta-llama/Llama-2-13b-chat-hf\n",
      "meta-llama/Llama-2-13b-hf\n",
      "meta-llama/Llama-2-70b-chat-hf\n",
      "meta-llama/Llama-2-70b-hf\n",
      "meta-llama/Llama-2-7b-chat-hf\n",
      "meta-llama/Llama-2-7b-hf\n",
      "meta-llama/Llama-3.2-1B\n",
      "meta-llama/Llama-3.2-1B-Instruct\n",
      "meta-llama/Llama-3.2-3B\n",
      "meta-llama/Llama-3.2-3B-Instruct\n",
      "meta-llama/Meta-Llama-3-70B\n",
      "meta-llama/Meta-Llama-3-70B-Instruct\n",
      "meta-llama/Meta-Llama-3-8B\n",
      "meta-llama/Meta-Llama-3-8B-Instruct\n",
      "meta-llama/Meta-Llama-3.1-405B\n",
      "meta-llama/Meta-Llama-3.1-405B-Instruct\n",
      "meta-llama/Meta-Llama-3.1-70B\n",
      "meta-llama/Meta-Llama-3.1-70B-Instruct\n",
      "meta-llama/Meta-Llama-3.1-8B\n",
      "meta-llama/Meta-Llama-3.1-8B-Instruct\n",
      "microsoft/phi-1_5\n",
      "microsoft/phi-2\n",
      "microsoft/Phi-3-mini-4k-instruct\n",
      "microsoft/Phi-3.5-mini-instruct\n",
      "mistralai/mathstral-7B-v0.1\n",
      "mistralai/Mistral-7B-Instruct-v0.1\n",
      "mistralai/Mistral-7B-Instruct-v0.2\n",
      "mistralai/Mistral-7B-Instruct-v0.3\n",
      "mistralai/Mistral-7B-v0.1\n",
      "mistralai/Mistral-7B-v0.3\n",
      "mistralai/Mistral-Large-Instruct-2407\n",
      "mistralai/Mixtral-8x7B-Instruct-v0.1\n",
      "mistralai/Mixtral-8x7B-v0.1\n",
      "NousResearch/Nous-Hermes-13b\n",
      "NousResearch/Nous-Hermes-llama-2-7b\n",
      "NousResearch/Nous-Hermes-Llama2-13b\n",
      "openlm-research/open_llama_13b\n",
      "openlm-research/open_llama_3b\n",
      "openlm-research/open_llama_7b\n",
      "stabilityai/FreeWilly2\n",
      "stabilityai/stable-code-3b\n",
      "stabilityai/stablecode-completion-alpha-3b\n",
      "stabilityai/stablecode-completion-alpha-3b-4k\n",
      "stabilityai/stablecode-instruct-alpha-3b\n",
      "stabilityai/stablelm-3b-4e1t\n",
      "stabilityai/stablelm-base-alpha-3b\n",
      "stabilityai/stablelm-base-alpha-7b\n",
      "stabilityai/stablelm-tuned-alpha-3b\n",
      "stabilityai/stablelm-tuned-alpha-7b\n",
      "stabilityai/stablelm-zephyr-3b\n",
      "tiiuae/falcon-180B\n",
      "tiiuae/falcon-180B-chat\n",
      "tiiuae/falcon-40b\n",
      "tiiuae/falcon-40b-instruct\n",
      "tiiuae/falcon-7b\n",
      "tiiuae/falcon-7b-instruct\n",
      "TinyLlama/TinyLlama-1.1B-Chat-v1.0\n",
      "TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T\n",
      "togethercomputer/LLaMA-2-7B-32K\n",
      "togethercomputer/RedPajama-INCITE-7B-Base\n",
      "togethercomputer/RedPajama-INCITE-7B-Chat\n",
      "togethercomputer/RedPajama-INCITE-7B-Instruct\n",
      "togethercomputer/RedPajama-INCITE-Base-3B-v1\n",
      "togethercomputer/RedPajama-INCITE-Base-7B-v0.1\n",
      "togethercomputer/RedPajama-INCITE-Chat-3B-v1\n",
      "togethercomputer/RedPajama-INCITE-Chat-7B-v0.1\n",
      "togethercomputer/RedPajama-INCITE-Instruct-3B-v1\n",
      "togethercomputer/RedPajama-INCITE-Instruct-7B-v0.1\n",
      "Trelis/Llama-2-7b-chat-hf-function-calling-v2\n",
      "unsloth/Mistral-7B-v0.2\n"
     ]
    }
   ],
   "source": [
    "!litgpt download list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2495037e-0068-49ad-9bed-0bcdc440727d",
   "metadata": {},
   "source": [
    "- We can then download an LLM via the following command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fb0c202d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting HF_HUB_ENABLE_HF_TRANSFER=1\n",
      "config.json: 100%|█████████████████████████████| 735/735 [00:00<00:00, 6.53MB/s]\n",
      "generation_config.json: 100%|██████████████████| 124/124 [00:00<00:00, 1.24MB/s]\n",
      "model-00001-of-00002.safetensors: 100%|████▉| 5.00G/5.00G [00:11<00:00, 452MB/s]\n",
      "model-00002-of-00002.safetensors: 100%|█████▉| 564M/564M [00:29<00:00, 19.4MB/s]\n",
      "model.safetensors.index.json: 100%|████████| 35.7k/35.7k [00:00<00:00, 21.8MB/s]\n",
      "tokenizer.json: 100%|███████████████████████| 2.11M/2.11M [00:02<00:00, 876kB/s]\n",
      "tokenizer_config.json: 100%|███████████████| 7.34k/7.34k [00:00<00:00, 35.3MB/s]\n",
      "Converting .safetensor files to PyTorch binaries (.bin)\n",
      "checkpoints/microsoft/phi-2/model-00002-of-00002.safetensors --> checkpoints/microsoft/phi-2/model-00002-of-00002.bin\n",
      "checkpoints/microsoft/phi-2/model-00001-of-00002.safetensors --> checkpoints/microsoft/phi-2/model-00001-of-00002.bin\n",
      "Converting checkpoint files to LitGPT format.\n",
      "{'checkpoint_dir': PosixPath('checkpoints/microsoft/phi-2'),\n",
      " 'debug_mode': False,\n",
      " 'dtype': None,\n",
      " 'model_name': None}\n",
      "Loading weights: model-00002-of-00002.bin: 100%|████████| 00:34<00:00,  2.89it/s\n",
      "Saving converted checkpoint to checkpoints/microsoft/phi-2\n"
     ]
    }
   ],
   "source": [
    "!litgpt download microsoft/phi-2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6caf5be0-4aa1-498f-b08a-68ff234cbea5",
   "metadata": {},
   "source": [
    "- And there's also a Python API to use the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e057edbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/LLMs/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/opt/conda/envs/LLMs/lib/python3.10/site-packages/litgpt/utils.py:613: UserWarning: The file size of checkpoints/microsoft/phi-2/lit_model.pth is over 4.2 GB. Using a model with more than 1B parameters on a CPU can be slow, it is recommended to switch to a GPU.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' The visual identity should reflect a sense of adventure, adventure, and whimsy, reflecting the playful and curious nature of llamas. The colors used in the design should be bold and earthy, such as shades of green, brown, and deep red'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from litgpt import LLM\n",
    "\n",
    "llm = LLM.load(\"microsoft/phi-2\")\n",
    "\n",
    "llm.generate(\"What do Llamas eat?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fc775d4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Llamas are herbivores and primarily eat grass, hay, and other plant material.\n"
     ]
    }
   ],
   "source": [
    "result = llm.generate(\"What do Llamas eat?\", stream=True, max_new_tokens=200)\n",
    "for e in result:\n",
    "    print(e, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "288158da",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "# Exercise 2: Download an LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2717b67f",
   "metadata": {},
   "source": [
    "- Download and try out an LLM of your own choice (recommendation: 7B parameters or smaller)\n",
    "- We will finetune the LLM in the next notebook\n",
    "- You can also try out the `litgpt chat` command from the terminal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28317a81",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "572323b4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
