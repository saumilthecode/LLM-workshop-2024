{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c024bfa4-1a7a-4751-b5a1-827225a3478b",
   "metadata": {
    "id": "c024bfa4-1a7a-4751-b5a1-827225a3478b"
   },
   "source": [
    "**LLM Workshop 2024 by Sebastian Raschka**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58b8c870-fb72-490e-8916-d8129bd5d1ff",
   "metadata": {
    "id": "58b8c870-fb72-490e-8916-d8129bd5d1ff"
   },
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "# 6) Instruction finetuning (part 3; benchmark evaluation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "013b3a3f-f300-4994-b704-624bdcd6371b",
   "metadata": {},
   "source": [
    "- In the previous notebook, we finetuned the LLM; in this notebook, we evaluate it using popular benchmark methods\n",
    "\n",
    "- There are 3 main types of model evaluation\n",
    "\n",
    "  1. MMLU-style Q&A\n",
    "  2. LLM-based automatic scoring\n",
    "  3. Human ratings by relative preference\n",
    "  \n",
    "  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfc0f1d9-f72b-4cf7-aef6-0407acf1a46e",
   "metadata": {},
   "source": [
    "<img src=\"figures/10.png\" width=800px>\n",
    "\n",
    "<img src=\"figures/11.png\" width=800px>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c389e66f-5463-4e40-820c-55d1e3eb5077",
   "metadata": {},
   "source": [
    "\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "\n",
    "<img src=\"figures/13.png\" width=800px>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0070e756",
   "metadata": {},
   "source": [
    "<img src=\"figures/14.png\" width=800px>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45464478-d7b1-4300-b251-a6e125c8ae52",
   "metadata": {},
   "source": [
    "## https://tatsu-lab.github.io/alpaca_eval/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de7ba804",
   "metadata": {},
   "source": [
    "<img src=\"figures/15.png\" width=800px>\n",
    "\n",
    "## https://chat.lmsys.org"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e084ce8a-bcba-4c67-ad49-0a645ce65eb7",
   "metadata": {
    "tags": []
   },
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "# 6.2 Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d0f6422-582f-4996-ae2a-dbacc9ebf86d",
   "metadata": {},
   "source": [
    "- In this notebook, we do an MMLU-style evaluation in LitGPT, which is based on the [EleutherAI LM Evaluation Harness](https://github.com/EleutherAI/lm-evaluation-harness)\n",
    "- There are hundreds if not thousands of benchmarks; using the command below, we filter for MMLU subsets, because running the evaluation on the whole MMLU dataset would take a very long time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67bb4cb5-e8ea-445b-8862-e7151c1544fc",
   "metadata": {},
   "source": [
    "- Let's say we are intrested in the `mmlu_philosophy` subset, se can evaluate the LLM on MMLU as follows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4edc913",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "# Exercise 3: Evaluate the finetuned LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "773a25be-5a02-477b-bfea-ffd53e44647b",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "# Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6cd718c4-0e83-4a83-84f8-59e3fc4c3404",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsupported `repo_id`: out/finetune/lora/final.\n",
      "If you are trying to download alternative weights for a supported model, please specify the corresponding model via the `--model_name` option, for example, `litgpt download NousResearch/Hermes-2-Pro-Llama-3-8B --model_name Llama-3-8B`.\n",
      "Alternatively, please choose a valid `repo_id` from the list of supported models, which can be obtained via `litgpt download list`.\n",
      "{'access_token': None,\n",
      " 'batch_size': 4,\n",
      " 'checkpoint_dir': PosixPath('checkpoints/out/finetune/lora/final'),\n",
      " 'device': None,\n",
      " 'dtype': None,\n",
      " 'force_conversion': False,\n",
      " 'limit': None,\n",
      " 'num_fewshot': None,\n",
      " 'out_dir': None,\n",
      " 'save_filepath': None,\n",
      " 'seed': 1234,\n",
      " 'tasks': 'mmlu_philosophy'}\n",
      "2024-09-29 04:26:24.494492: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-09-29 04:26:24.618040: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-09-29 04:26:24.647113: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-09-29 04:26:24.829903: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-09-29 04:26:26.052621: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "{'checkpoint_dir': PosixPath('checkpoints/out/finetune/lora/final'),\n",
      " 'output_dir': PosixPath('checkpoints/out/finetune/lora/final/evaluate')}\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/envs/LLMs/bin/litgpt\", line 8, in <module>\n",
      "    sys.exit(main())\n",
      "  File \"/opt/conda/envs/LLMs/lib/python3.10/site-packages/litgpt/__main__.py\", line 71, in main\n",
      "    CLI(parser_data)\n",
      "  File \"/opt/conda/envs/LLMs/lib/python3.10/site-packages/jsonargparse/_cli.py\", line 119, in CLI\n",
      "    return _run_component(component, init.get(subcommand))\n",
      "  File \"/opt/conda/envs/LLMs/lib/python3.10/site-packages/jsonargparse/_cli.py\", line 204, in _run_component\n",
      "    return component(**cfg)\n",
      "  File \"/opt/conda/envs/LLMs/lib/python3.10/site-packages/litgpt/eval/evaluate.py\", line 95, in convert_and_evaluate\n",
      "    convert_lit_checkpoint(checkpoint_dir=checkpoint_dir, output_dir=out_dir)\n",
      "  File \"/opt/conda/envs/LLMs/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/opt/conda/envs/LLMs/lib/python3.10/site-packages/litgpt/scripts/convert_lit_checkpoint.py\", line 333, in convert_lit_checkpoint\n",
      "    config = Config.from_file(checkpoint_dir / \"model_config.yaml\")\n",
      "  File \"/opt/conda/envs/LLMs/lib/python3.10/site-packages/litgpt/config.py\", line 126, in from_file\n",
      "    with open(path, encoding=\"utf-8\") as fp:\n",
      "FileNotFoundError: [Errno 2] No such file or directory: 'checkpoints/out/finetune/lora/final/model_config.yaml'\n"
     ]
    }
   ],
   "source": [
    "!litgpt evaluate out/finetune/lora/final --tasks \"mmlu_philosophy\" --batch_size 4"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "V100",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
